# -*- coding: utf-8 -*-
"""
å‘é‡æœåŠ¡æ¨¡å—

- FAISS å‘é‡å­˜å‚¨
- LlamaIndex RAG
- DeepSeek Chat API

é—®é¢˜è¯Šæ–­ï¼š
1. æ–‡æ¡£æ’å…¥æˆåŠŸï¼ˆ680ä¸ªå‘é‡ï¼‰
2. æŸ¥è¯¢è¿”å›ç©ºï¼Œå¯èƒ½åŸå› ï¼š
   - DeepSeek APIè°ƒç”¨å¤±è´¥
   - æŸ¥è¯¢å¤„ç†é€»è¾‘é—®é¢˜
   - å‘é‡ç´¢å¼•æ„å»ºé—®é¢˜
"""

import os
from typing import Any, Mapping, List

from openai import OpenAI
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.settings import Settings
from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.embeddings import MockEmbedding


# =========================
# é…ç½®
# =========================

VECTOR_STORE_PATH = "data/vector_db"
os.makedirs(VECTOR_STORE_PATH, exist_ok=True)

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_BASE = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

if not DEEPSEEK_API_KEY:
    raise ValueError("è¯·é…ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")


# =========================
# DeepSeek LLM å®ç°
# =========================

class DeepSeekLLM(CustomLLM):
    """DeepSeek Chat LLM (LlamaIndex CustomLLM é€‚é…)"""

    def __init__(self, api_key: str, base_url: str, model: str):
        # âš ï¸ å¿…é¡»æœ€å…ˆè°ƒç”¨
        super().__init__()

        self._api_key = api_key
        self._base_url = base_url
        self._model = model
        self._client = OpenAI(api_key=api_key, base_url=base_url)

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            model_name=self._model,
            context_window=32768,
            num_output=4096,
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> str:
        """éæµå¼ç”Ÿæˆï¼ˆQueryEngine å®é™…è°ƒç”¨çš„æ–¹æ³•ï¼‰"""
        try:
            print(f"ğŸ” DeepSeek APIè°ƒç”¨ - æç¤ºè¯é•¿åº¦: {len(prompt)}")
            print(f"ğŸ” æç¤ºè¯å‰200å­—ç¬¦: {prompt[:200]}...")
            
            resp = self._client.chat.completions.create(
                model=self._model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å¯é çš„ AI åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )

            text = resp.choices[0].message.content
            print(f"âœ… DeepSeek APIå“åº”æˆåŠŸ - å“åº”é•¿åº¦: {len(text)}")
            print(f"âœ… å“åº”å‰200å­—ç¬¦: {text[:200]}...")
            return text
        except Exception as e:
            print(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}"

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any):
        """ç®€åŒ–å¤„ç†ï¼šå…ˆä¸çœŸæ­£åšæµå¼"""
        yield self.complete(prompt, **kwargs)


# =========================
# LlamaIndex å…¨å±€è®¾ç½®
# =========================

Settings.llm = DeepSeekLLM(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_API_BASE,
    model=DEEPSEEK_MODEL,
)

# âš ï¸ MockEmbedding åªé€‚åˆ demo / è°ƒè¯•
Settings.embed_model = MockEmbedding(embed_dim=384)

print("âœ… DeepSeek LLM åˆå§‹åŒ–å®Œæˆï¼ˆMockEmbeddingï¼‰")


# =========================
# å‘é‡ç´¢å¼•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
# =========================

index: VectorStoreIndex | None = None
storage_context: StorageContext | None = None


# =========================
# å¯¹å¤–å‡½æ•°
# =========================

def _load_or_create_index():
    global index, storage_context

    if index is not None:
        return

    try:
        storage_context = StorageContext.from_defaults(
            persist_dir=VECTOR_STORE_PATH
        )
        index = VectorStoreIndex.from_documents(
            [],
            storage_context=storage_context,
        )
        print("ğŸ“¦ å·²åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼•")
    except Exception:
        storage_context = StorageContext.from_defaults()
        index = VectorStoreIndex([], storage_context=storage_context)
        print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")


def add_documents_to_index(docs: List):
    """
    æ·»åŠ æ–‡æ¡£åˆ°å‘é‡ç´¢å¼•
    docs: List[llama_index.core.schema.Document]
    """
    _load_or_create_index()

    if not docs:
        return

    for doc in docs:
        index.insert(doc)

    index.storage_context.persist(persist_dir=VECTOR_STORE_PATH)
    print(f"âœ… å·²æ’å…¥ {len(docs)} ä¸ªæ–‡æ¡£")


def query_vector_store(query_text: str, top_k: int = 5) -> str:
    """
    å‘é‡æŸ¥è¯¢æ¥å£
    
    é—®é¢˜è¯Šæ–­æ­¥éª¤ï¼š
    1. æ£€æŸ¥å‘é‡ç´¢å¼•çŠ¶æ€
    2. æ£€æŸ¥DeepSeek APIè°ƒç”¨
    3. æ£€æŸ¥æŸ¥è¯¢å¤„ç†æµç¨‹
    """
    print(f"ğŸ” å¼€å§‹æŸ¥è¯¢å¤„ç† - æŸ¥è¯¢å†…å®¹: {query_text}")
    
    _load_or_create_index()

    # æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦æœ‰æ–‡æ¡£
    doc_count = len(index.docstore.docs)
    print(f"ğŸ“Š å‘é‡ç´¢å¼•ä¸­ç°æœ‰æ–‡æ¡£æ•°é‡: {doc_count}")
    
    if doc_count == 0:
        return "é”™è¯¯ï¼šå‘é‡ç´¢å¼•ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ PDFæ–‡æ¡£"

    try:
        print(f"ğŸ” åˆ›å»ºæŸ¥è¯¢å¼•æ“ - top_k: {top_k}")
        query_engine = index.as_query_engine(
            similarity_top_k=top_k
        )
        
        print(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢...")
        response = query_engine.query(query_text)
        
        # ç›´æ¥è·å–å“åº”å†…å®¹ - ä½¿ç”¨response.responseå±æ€§
        if hasattr(response, 'response') and response.response:
            actual_response = response.response
            print(f"âœ… è·å–åˆ°response.responseå†…å®¹")
            print(f"ğŸ” response.responseç±»å‹: {type(actual_response)}")
            print(f"ğŸ” response.responseå†…å®¹é•¿åº¦: {len(str(actual_response))}")
            print(f"ğŸ” response.responseå†…å®¹: {str(actual_response)[:500]}...")
            
            response_str = str(actual_response)
        else:
            # å¦‚æœresponse.responseä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å±æ€§
            print(f"ğŸ” æŸ¥è¯¢å“åº”ç±»å‹: {type(response)}")
            print(f"ğŸ” å“åº”å¯¹è±¡å±æ€§: {[attr for attr in dir(response) if not attr.startswith('_')]}")
            
            # å°è¯•ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            response_str = str(response)
            print(f"ğŸ” str(response)é•¿åº¦: {len(response_str)}")
            print(f"ğŸ” str(response)å†…å®¹: {response_str[:500]}...")
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
        if not response_str or response_str.strip() == "" or response_str.strip() == "Empty Response":
            print("âš ï¸ å“åº”ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ£€ç´¢å™¨æ£€æŸ¥æ–‡æ¡£åŒ¹é…æƒ…å†µ")
            
            # ä½¿ç”¨æ£€ç´¢å™¨æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
            retriever = index.as_retriever(similarity_top_k=top_k)
            retrieved_nodes = retriever.retrieve(query_text)
            print(f"ğŸ” æ£€ç´¢å™¨æ‰¾åˆ°æ–‡æ¡£æ•°é‡: {len(retrieved_nodes)}")
            
            if retrieved_nodes:
                print("âœ… æ£€ç´¢å™¨æ‰¾åˆ°äº†ç›¸å…³æ–‡æ¡£ï¼Œä½†LLMè¿”å›ç©ºå“åº”")
                # æ„å»ºç®€å•çš„æ–‡æ¡£æ‘˜è¦
                summary_parts = ["æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œç›¸å…³å†…å®¹å¦‚ä¸‹ï¼š"]
                for i, node in enumerate(retrieved_nodes[:3], 1):
                    preview = node.text[:300] + "..." if len(node.text) > 300 else node.text
                    summary_parts.append(f"\n{i}. {preview}")
                return "\n".join(summary_parts)
            else:
                print("âŒ æ£€ç´¢å™¨ä¹Ÿæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚è¯·å°è¯•ç”¨ä¸åŒçš„å…³é”®è¯æé—®ã€‚"
        
        return response_str
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return f"æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"
