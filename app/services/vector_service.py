# -*- coding: utf-8 -*-
"""
å‘é‡æœåŠ¡æ¨¡å—

- FAISS å‘é‡å­˜å‚¨
- LlamaIndex RAG
- DeepSeek Chat API
"""

import os
from typing import Any, Mapping, List

from openai import OpenAI
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.settings import Settings
from llama_index.core.llms import CustomLLM, CompletionResponse
from llama_index.core.llms.callbacks import llm_completion_callback
from llama_index.core.embeddings import MockEmbedding


# =========================
# é…ç½®
# =========================

VECTOR_STORE_PATH = "data/vector_db"
os.makedirs(VECTOR_STORE_PATH, exist_ok=True)

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_API_BASE = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

if not DEEPSEEK_API_KEY:
    raise ValueError("è¯·é…ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")


# =========================
# DeepSeek LLM å®ç°
# =========================

class DeepSeekLLM(CustomLLM):
    """DeepSeek Chat LLM (LlamaIndex CustomLLM é€‚é…)"""

    def __init__(self, api_key: str, base_url: str, model: str):
        # âš ï¸ å¿…é¡»æœ€å…ˆè°ƒç”¨
        super().__init__()

        self._api_key = api_key
        self._base_url = base_url
        self._model = model
        self._client = OpenAI(api_key=api_key, base_url=base_url)

    @property
    def metadata(self) -> Mapping[str, Any]:
        return {
            "model": self._model,
            "context_window": 32768,
            "num_output": 4096,
        }

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> str:
        """éæµå¼ç”Ÿæˆï¼ˆQueryEngine å®é™…è°ƒç”¨çš„æ–¹æ³•ï¼‰"""
        resp = self._client.chat.completions.create(
            model=self._model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å¯é çš„ AI åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        text = resp.choices[0].message.content
        return text

    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any):
        """ç®€åŒ–å¤„ç†ï¼šå…ˆä¸çœŸæ­£åšæµå¼"""
        yield self.complete(prompt, **kwargs)


# =========================
# LlamaIndex å…¨å±€è®¾ç½®
# =========================

Settings.llm = DeepSeekLLM(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_API_BASE,
    model=DEEPSEEK_MODEL,
)

# âš ï¸ MockEmbedding åªé€‚åˆ demo / è°ƒè¯•
Settings.embed_model = MockEmbedding(embed_dim=384)

print("âœ… DeepSeek LLM åˆå§‹åŒ–å®Œæˆï¼ˆMockEmbeddingï¼‰")


# =========================
# å‘é‡ç´¢å¼•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
# =========================

index: VectorStoreIndex | None = None
storage_context: StorageContext | None = None


# =========================
# å¯¹å¤–å‡½æ•°
# =========================

def _load_or_create_index():
    global index, storage_context

    if index is not None:
        return

    try:
        storage_context = StorageContext.from_defaults(
            persist_dir=VECTOR_STORE_PATH
        )
        index = VectorStoreIndex.from_documents(
            [],
            storage_context=storage_context,
        )
        print("ğŸ“¦ å·²åŠ è½½æœ¬åœ°å‘é‡ç´¢å¼•")
    except Exception:
        storage_context = StorageContext.from_defaults()
        index = VectorStoreIndex([], storage_context=storage_context)
        print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")


def add_documents_to_index(docs: List):
    """
    æ·»åŠ æ–‡æ¡£åˆ°å‘é‡ç´¢å¼•
    docs: List[llama_index.core.schema.Document]
    """
    _load_or_create_index()

    if not docs:
        return

    for doc in docs:
        index.insert(doc)

    index.storage_context.persist(persist_dir=VECTOR_STORE_PATH)
    print(f"âœ… å·²æ’å…¥ {len(docs)} ä¸ªæ–‡æ¡£")


def query_vector_store(query_text: str, top_k: int = 5) -> str:
    """
    å‘é‡æŸ¥è¯¢æ¥å£
    """
    _load_or_create_index()

    if not index.docstore.docs:
        return "é”™è¯¯ï¼šå‘é‡ç´¢å¼•ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£"

    try:
        query_engine = index.as_query_engine(
            similarity_top_k=top_k
        )
        response = query_engine.query(query_text)
        return str(response)
    except Exception as e:
        return f"æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"
